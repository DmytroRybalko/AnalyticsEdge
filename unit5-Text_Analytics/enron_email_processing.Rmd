---
title: "Enron Data"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tm)
library(caTools)
library(rpart)
library(rpart.plot)
library(ROCR)
```

# Load data

```{r}
emails <- read.csv("data/energy_bids.csv", stringsAsFactors = FALSE)
str(emails)
```

So let's take a look at a few example emails in the data set, starting with the first one.  
```{r}
strwrap(emails$email[1])
```
So let's take a look now at this email, now that it's a lot easier to read. We can see just by parsing through the first couple of lines that this is an email that's talking about a new working paper, "The Environmental Challenges and Opportunities in the Evolving North American Electricity Market" is the name of the paper. And it's being released by the Commission for Environmental Cooperation, or CEC. So while this certainly deals with electricity markets, it doesn't have to do with energy schedules or bids. So it is not responsive to our query.  
```{r}
emails$responsive[1]
```
Look into second email:
```{r}
strwrap(emails$email[2])
```
```{r}
emails$responsive[2]
```
So now let's look at the breakdown of the number of emails that are responsive to our query using the table function.
```{r}
table(emails$responsive)
```
Let's create text corpus
```{r}
corpus <- VCorpus(VectorSource(emails$email))
```
Look into singe value:
```{r}
strwrap(corpus[[1]]$content)
```

So now we're ready to preprocess the corpus using the tm_map function. So first, we'll convert the corpus to lowercase using tm_map and the tolower function. 
```{r}
corpus <- tm_map(corpus, content_transformer(tolower))
```
Moving punctuation:
```{r}
corpus <- tm_map(corpus, removePunctuation)
```
Remove stopwords:
```{r}
corpus <- tm_map(corpus, removeWords, stopwords("english"))
```
Stemming words:
```{r}
corpus <- tm_map(corpus, stemDocument)
```
Look at first email
```{r}
corpus[[1]]$content
```
# Create Term Document Matrix

```{r}
dtm <- DocumentTermMatrix(corpus)
dtm
```
Remove terms that not appear too often. And we're going to have to determine the sparsity, so we'll say that we'll remove any term that doesn't appear in at least 3% of the documents.
```{r}
dtm <- removeSparseTerms(dtm, 0.97)
dtm
```

Create data frame
```{r}
labeledTerms <- as.data.frame(as.matrix(dtm))
```
# Add in the outcome variable
```{r}
labeledTerms$responsive <- emails$responsive
str(labeledTerms)
```
# Building Models  

Split the data
```{r}
set.seed(144)

spl <- sample.split(labeledTerms$responsive, 0.7)

train <- subset(labeledTerms, spl == TRUE)
test <- subset(labeledTerms, spl == FALSE)
```

Build a CART model
```{r}
emailCART <- rpart(responsive ~ ., data = train, method = "class")
prp(emailCART)
```

There we go. So we can see at the very top is the word California. If California appears at least twice in an email, we're going to take the right part over here and predict that a document is responsive. It's somewhat unsurprising that California shows up, because we know that Enron had a heavy involvement in the California energy markets. So further down the tree, we see a number of other terms that we could plausibly expect to be related to energy bids and energy scheduling, like system, demand, bid, and gas. Down here at the bottom is Jeff, which is perhaps a reference to Enron's CEO, Jeff Skillings, who ended up actually being jailed for his involvement in the fraud at the company.  
  
# Evaluting the Model

Now that we've trained a model, we need to evaluate it on the test set.
```{r}
pred <- predict(emailCART, newdata = test)
pred[1:10,]
```
So the left column here is the predicted probability of the document being non-responsive. And the right column is the predicted probability
of the document being responsive. So in our case, we want to extract the predicted probability of the document being responsive. So we're looking for the rightmost column. So we'll create an object called pred.prob. And we'll select the rightmost or second column.
```{r}
pred.prob <- pred[,2]
```

Compute accuracy
```{r}
table(test$responsive, pred.prob >= 0.5)
```
What we can see here is that in 195 cases, we predict false when the left column and the true outcome was zero, non-responsive. So we were correct. And in another 25, we correctly identified a responsive document. In 20 cases, we identified a document as responsive, but it was actually non-responsive. And in 17, the opposite happened. We identified a document as non-responsive, but it actually was responsive.
```{r}
(195+25)/(195+25+17+20)
```
And now we want to compare ourselves to the accuracy of the baseline model. As we've already established, the baseline model is always going to predict the document is non-responsive.
```{r}
table(test$responsive)
```
Accuracy
```{r}
215/(215+42)
```

So we see just a small improvement in accuracy using the CART model, which, as we know, is a common case in unbalanced data sets. However, as in most document retrieval applications, there are uneven costs for different types of errors here. Typically, a human will still have to manually review all of the predicted responsive documents to make sure they are actually responsive. Therefore, if we have a false positive,
in which a non-responsive document is labeled as responsive, the mistake translates to a bit of additional work in the manual review process but no further harm, since the manual review process will remove this erroneous result. But on the other hand, if we have a false negative,
in which a responsive document is labeled as non-responsive by our model, we will miss the document entirely in our predictive coding process. Therefore, we're going to assign a higher cost to false negatives than to false positives, which makes this a good time to look at other cut-offs on our ROC curve.

# ROC curve 

Now let's look at the ROC curve so we can understand the performance of our model at different cutoffs.
```{r}
predROCR <- prediction(pred.prob, test$responsive)
```
So create something called perfROCR = performance(predROCR, "tpr", "fpr").
```{r}
perfROCR <- performance(predROCR, "tpr", "fpr")
plot(perfROCR, colorize = TRUE)
```
Now, of course, the best cutoff to select is entirely dependent on the costs assigned by the decision maker to false positives and true positives. However, again, we do favor cutoffs that give us a high sensitivity. We want to identify a large number of the responsive
documents. So something that might look promising might be a point right around here, in this part of the curve, where we have a true positive rate of around 70%, meaning that we're getting about 70% of all the responsive documents, and a false positive rate of about 20%, meaning that we're making mistakes and accidentally identifying as responsive 20% of the non-responsive documents.  

Now, since, typically, the vast majority of documents are non-responsive, operating at this cutoff would result, perhaps, in a large decrease
in the amount of manual effort needed in the eDiscovery process. And we can see from the blue color of the plot at this particular location
that we're looking at a threshold around maybe 0.15 or so, significantly lower than 50%, which is definitely what we would expect since we favor false positives to false negatives.  
  
Compute AUC
```{r}
performance(predROCR, "auc")@y.values
```
We can see that we have an AUC in the test set of 79.4%, which means that our model can differentiate between a randomly selected responsive and non-responsive document about 80% of the time.  
  












